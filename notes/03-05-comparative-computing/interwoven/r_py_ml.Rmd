---
title: "Combining `R` and `Python` to conduct machine learning: Predicting wine quality"
author: Simon Heuberger
date: "`r format(Sys.time(), '%B %d, %Y')`"
output: html_document
---


```{r include=FALSE}
knitr::opts_chunk$set(
    message = FALSE,  # toggle off message output 
    warning = FALSE)  # toggle off warning output
library(reticulate) # to use Python within R
library(tidyverse) # data wrangling
library(tidyquant) # for theme_tq()
library(plotly) # for interactive plots
```


Connect `RStudio` with the `Python 3` Anaconda installation.

\vspace{0.2cm}

```{r}
conda_list()
use_condaenv("anaconda3")
```


Install the needed `Python` libraries.

\vspace{0.2cm}

```{python}
import numpy as np # numerical computing library
import pandas as pd # wrangling of data

# The rest is from sklearn, an excellent machine learning library
from sklearn.model_selection import train_test_split
from sklearn import preprocessing
from sklearn.ensemble import RandomForestRegressor
from sklearn.pipeline import make_pipeline
from sklearn.model_selection import GridSearchCV
```


Load and look at the data in `Python`.

\vspace{0.2cm}

```{python}
dataset_url = 'http://mlr.cs.umass.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv'
data = pd.read_csv(dataset_url, sep = ";")
print(data.head())
```


It doesn't really give us that much information. `R` is better here.

\vspace{0.2cm}

```{r}
py$data %>%
    as_tibble() %>%
    glimpse()
```


What can we see? All features are numeric (`dbl`). The target of our predictions is `quality`. The predictors are `fixed acidity`, `chlorides`, `pH` etc. Let's conduct the machine learning estimation with `Python`, to take advantage of the `sklearn` library.

\vspace{0.2cm}

```{python results='hide'}
# Separate data into X (predictive) and y (target) variables
y = data.quality
X = data.drop("quality", axis=1)

# Split X and y into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(
  X, y,
  test_size    = 0.2,
  random_state = 123,
  stratify     = y
)

# Preprocess by calculating the scale from X_train
scaler = preprocessing.StandardScaler().fit(X_train)

# Apply transformation to X_test
X_test_scaled = scaler.transform(X_test)

# Setup an ML pipeline where numeric values are scaled and a random forest
# regression model is created
pipeline = make_pipeline(
    preprocessing.StandardScaler(),
    RandomForestRegressor(n_estimators = 100)
)

# We want to get the optimal combination of parameters. Set up a hyperparameters
# object that has the combination of attributes we want to change
hyperparameters = {
    "randomforestregressor__max_features" : ["auto", "sqrt", "log2"],
    "randomforestregressor__max_depth"    : [None, 5, 3, 1]
}

# Then apply grid search with cross validation
clf = GridSearchCV(estimator = pipeline, param_grid = hyperparameters, cv = 10)
clf.fit(X_train, y_train)

# Make predictions
y_pred = clf.predict(X_test)
```


Is our model any good? Let's visualize things in `R` to find out.

\vspace{0.2cm}

```{r}
# Manipulate data for ggplot
results_tbl <- tibble(
    y_test = py$y_test,
    y_pred = py$y_pred
    ) %>%
    rowid_to_column() %>%
    arrange(y_test) %>%
    mutate(rowid = as_factor(as.character(rowid))) %>%
    rowid_to_column("sorted_rowid") %>%
    gather(key = "key", value = "value", -c(rowid, sorted_rowid))

# Make ggplot
p <- results_tbl %>%
  ggplot(aes(sorted_rowid, value, color = key)) +
  geom_point(alpha = 0.5) +
  geom_smooth() +
  theme_tq() +
  scale_color_tq() +
  labs(
    title = "Prediction Versus Actual",
    subtitle = "Wine Quality Level",
    x = "Sorted RowID", y = "Quality Level"
    )

p
```


The above is a static version. What if we would like to be able to analyze the plot interactively in our document?

\vspace{0.2cm}

```{r}
ggplotly(p)
```

Note that the interactive features of `plotly` only work in `html`. It will show as a static document in a `pdf`. So what about our model? The red and black lines look pretty good, but the gap is clearly wider at both ends. The model seems to be predicting low wine quality levels higher than they should be and high wine quality levels lower than they should be. Thus, our model still needs improving to more accurately predict very bad and very good wines.

